he Oxford Classical Dictionary, 1949 edition, edited by Cary and Ross. This is one of the foundational reference works for the entire domain Chrystallum is building on. A few thoughts.

What this is as a source
The OCD is the authoritative single-volume reference for classical antiquity — structured exactly the way Chrystallum thinks about knowledge. Every entry is a named entity with attestations, cross-references to other entities, and implicit confidence levels expressed through scholarly hedging. "Probably," "perhaps," "according to Livy" — these are IGAR in prose form. The editors made exactly the kind of contested-claim decisions that Chrystallum's epistemological framework was designed to hold explicitly.
The 1949 edition is the first major edition — lean, authoritative, pre-revisionist in some areas but structurally sound. The entity coverage is dense: persons, places, institutions, events, legal concepts, military formations, religious practices.

What it means for Chrystallum specifically
As a text corpus it is the ideal test case for the browser extension use case you described. Someone reading an OCD entry on Sulla encounters a web of named entities — people, places, offices, battles, laws — all of which should be resolvable to Chrystallum QIDs. The extension should light up on every proper noun and offer the graph layer. That is exactly the use case, and the OCD is dense enough that almost every sentence would trigger something.
As a structured source it is worth considering as a bibliography node in the graph itself. Every OCD entry is a DESCRIBED_BY_SOURCE edge waiting to be made. The P1343 canonicalization you just completed — described by source at 3,226 edges — is the right property. If OCD entries can be resolved to Wikidata items for the source work, those edges would connect Chrystallum entities to one of the most authoritative secondary sources in the field.
As a content source for the SFA layer, the OCD entries encode exactly the kind of compressed scholarly judgment that the Faculty Seminar model should be able to reason from. An SFA reading the Caesar entry would extract claims, note the confidence language, identify the primary sources cited, and propose graph edges with appropriate framework types. This is a concrete test case for whether the claim ingestion pipeline works on real scholarly text.

The practical question
The full text is available — 7.3MB plain text download. The question is whether you are thinking about this as a manual reference while building, as a corpus for automated entity extraction, or as a source to formally ingest into the graph as a bibliography node with structured claims. All three are valid but they are different amounts of work. The first is free. The second requires the claim ingestion pipeline to be further along. The third is a discrete task — create the OCD bibliography node, identify which Chrystallum entities have OCD entries, stamp DESCRIBED_BY_SOURCE edges — that could be done now against the existing graph without waiting for the SFA layer.
What were you thinking when you sent it?two or more things. one as a reference source for agents - it is a canonical work. Two within the subject domain, indentify subjects, people other aspects that might enrich our taxonomy7:06 PMBoth are the right instincts. Let me think through each properly.

As a reference source for agents
The OCD is exactly the kind of canonical authority that changes how an SFA reasons. When an SFA is generating a claim about Sulla and it can ground that claim against an OCD entry, the confidence posture changes — not because the OCD is infallible but because it represents the distilled judgment of the field at a known point in time. The SFA can say "Wikidata asserts X, OCD 1949 asserts Y, they diverge on Z" and that divergence is itself a meaningful claim about the state of scholarly knowledge.
The practical implementation is the OCD as a BIBLIOGRAPHY node in the graph — a single entity with QID, edition metadata, and editor attribution — and then individual entry-level nodes hanging off it. Each entry becomes a SOURCE_ENTRY node with its own identity, linked to the entities it covers via DESCRIBED_BY_SOURCE and to the claims it makes via the Attribution relationship types already in the registry. The SFA retrieves relevant entries before generating claims rather than reasoning from training knowledge alone. That is grounded reasoning rather than parametric recall, which is architecturally important for a system that needs to be epistemically honest about where its claims come from.
The 1949 edition has a specific limitation worth noting for the agents — it predates significant epigraphic discoveries and the systematic development of prosopography as a method. The Prosopographia Imperii Romani and the later LGPN work post-date it. So for persons especially, OCD 1949 should be flagged in the agent's reasoning as authoritative for broad factual claims but potentially superseded on individual identifications. That is a metadata property on the bibliography node itself — authority_scope, superseded_by, date_limitations — which the SFA can read and apply as a confidence modifier.

As a taxonomy enrichment source
This is the more immediately valuable use for where Chrystallum is right now. The OCD's entry structure is effectively a worked example of the SubjectConcept hierarchy you have been building. The editors made decisions about how to organize the domain — what deserves its own entry, what gets subsumed under a broader entry, what cross-references are essential — that encode decades of scholarly consensus about the conceptual structure of the ancient world.
Concretely, the OCD has entry categories that map directly onto gaps in the current taxonomy. Legal concepts — Roman law has an enormous entry apparatus covering specific statutes, legal procedures, offices with legal dimensions, constitutional concepts. The current registry has Legal relationship types but the SubjectConcept hierarchy for legal institutions is underdeveloped relative to the military and political facets. The OCD entry list would surface the concepts that matter — provocatio, lex, senatus consultum, intercessio, imperium — as SubjectConcept candidates with their own hierarchy.
Religious concepts similarly. The current graph has Q337547 (Public ritual) as a trustworthy cluster but Roman religion is far more structured than that single anchor suggests. The OCD has entries for specific cults, priesthoods, sacred spaces, ritual calendars, divine attributions. These are SubjectConcepts with BROADER_THAN relationships — flamines are a type of priesthood, the Vestals are a specific priestly college, the Lupercalia is a type of public ritual. The OCD entry structure gives you the hierarchy for free if you read it as a taxonomy rather than as a reference work.
The material culture and status markers gap identified earlier in the architecture — the missing SIGNIFIES relationship, the missing entity types for garments and objects as political signals — the OCD has entries for specific items: toga praetexta, fasces, corona triumphalis, lituus. Each of these is an entity with a SubjectConcept membership and a SIGNIFIES relationship to a status or office. The OCD entries tell you what those things signified and to whom, which is exactly the semantic load the graph needs.

The practical path
The full text extraction is the first step. The 7.3MB plain text from the Archive is OCR output from a 1949 scan, so it will have errors, but the entry structure — headings, cross-references, Latin terms — should be recoverable with a parsing pass.
From the extracted text, two immediate outputs are worth building. First, an entry list — every OCD headword as a potential entity, with its entry text and cross-references. This becomes the source for SubjectConcept gap analysis: which headwords do not have corresponding nodes in the current graph, which do but with thin coverage, which cross-reference structures suggest missing BROADER_THAN edges.
Second, a cross-reference graph from the OCD itself — entry A refers to entries B, C, D. That reference structure is a scholar-curated relationship graph. It is not the same as the Chrystallum relationship types but it is a signal about which entities are conceptually proximate. Sulla's entry cross-references Marius, the proscriptions, the Social War, the constitution. That is a salience signal for the reader persona — these are the connections that matter for understanding Sulla, as judged by the OCD editors.
The entry list and cross-reference graph can be built before the SFA layer is ready and fed into the SubjectConcept gap analysis as a validation against an external authoritative source. Where Chrystallum's SubjectConcept hierarchy agrees with the OCD's conceptual structure, confidence increases. Where they diverge, that divergence is worth examining — either the OCD is missing something the graph has discovered, or the graph has a gap the OCD reveals.

One caution
The 1949 edition is out of copyright and freely available, which makes it clean to use as a data source. But the OCD has had subsequent editions — 2nd edition 1970, 3rd edition 1996, 4th edition 2012 — each substantially revised. The 1949 edition reflects scholarship up to approximately the mid-1940s. For the Roman Republic specifically this is largely fine — the major source analysis and chronological framework were established by then — but for individual person identifications and for anything touching epigraphy, the later editions incorporate material the 1949 does not have. Worth documenting on the bibliography node that this is the 1949 edition specifically and that later editions exist with updated scholarship, so agents know to treat it as authoritative but not final.we have not talked about wikipeda. what if those ocd enteries could mappend to either wikipedia/data7:09 PMThis is the connective tissue that makes everything else more powerful. Let me think through it properly.

What the mapping actually gives you
The OCD entry is the authoritative scholarly judgment. The Wikipedia article is the accessible narrative. The Wikidata QID is the machine-readable identity. If you can align all three — OCD headword → Wikipedia article → Wikidata QID — you get a three-layer resolution for every entity in the domain.
The OCD says "this concept exists and here is what scholars agree about it." Wikipedia says "here is how this concept is explained to a general reader." Wikidata says "here is the unique identifier that connects this concept to everything else in the linked data universe." Chrystallum's graph node is the fourth layer — "here is how this concept connects to the rest of the Roman Republic knowledge graph with full epistemological provenance."
That four-layer resolution is what the browser extension needs to work properly. When a user reads a Wikipedia article on Sulla, the extension can pull the Wikidata QID from the Wikipedia page directly — Wikipedia embeds Wikidata QIDs in its page metadata. The QID resolves to the Chrystallum entity. The Chrystallum entity links back to the OCD entry as an authoritative source. The circle closes.

The mapping problem and why it is more tractable than it looks
The OCD headwords are Latin or Latinized Greek names and concepts. Wikipedia article titles for classical subjects tend to follow the same conventions — Sulla, not Lucius Cornelius Sulla Felix on the article title, though the full name appears in the lead. Wikidata items for well-known Roman figures already have P1343 (described by source) statements pointing to the OCD in many cases, because editors have been adding those links for years.
So for major figures and well-known concepts, the mapping may already exist in Wikidata. A SPARQL query against Wikidata for items where P1343 value includes the OCD would surface every entity Wikidata already knows is in the OCD. That is your seed mapping — the entities where the link is already made. From there the gap is the long tail — minor figures, specific legal concepts, obscure places — where the Wikidata editors have not yet added the OCD reference.
The SPARQL query would look roughly like:
sparqlSELECT ?item ?itemLabel ?ocdRef WHERE {
  ?item wdt:P1343 ?source .
  ?source wdt:P31 wd:Q13219454 .
  OPTIONAL { ?item wdt:P1343 ?ocdRef }
  SERVICE wikibase:label { 
    bd:serviceParam wikibase:language "en". 
  }
}
Though the exact OCD Wikidata item QID would need to be identified first — it exists, the OCD is in Wikidata as a bibliographic item.

The enrichment flow this enables
Once the three-way mapping exists — OCD entry, Wikipedia article, Wikidata QID — several things become possible that are not possible today.
The SubjectConcept gap analysis becomes concrete. Every OCD entry that does not resolve to a Wikidata QID is a concept the linked data world has not yet modeled. Every OCD entry that resolves to a Wikidata QID but has no Chrystallum SubjectConcept is a gap in the Chrystallum taxonomy. The mapping surfaces both gaps simultaneously.
The Wikipedia article text becomes a readable description layer for Chrystallum entities. Right now the graph has QIDs and labels but no narrative context. Wikipedia's lead paragraph is the accessible summary of what an entity is. That summary, attached to the graph node, is what the reader persona sees when they land on an entity — not a data record but a human-readable orientation, with the graph connections below it.
The Wikipedia cross-reference structure adds another salience signal alongside the OCD cross-references. If both the OCD and the Wikipedia article on Sulla cross-reference Marius, that double signal is stronger evidence of conceptual proximity than either source alone. The reader persona's graph navigation should surface the connections that multiple authoritative sources agree are central.
Wikipedia's categories add a third taxonomy layer. Wikipedia categorizes Sulla under Roman dictators, Roman generals of the Republic, 1st-century BC Roman politicians, and so on. Those categories are imperfect but they encode community consensus about how to classify entities. Cross-referenced against the OCD entry structure and the Chrystallum SubjectConcept hierarchy, they help validate or challenge classification decisions.

The taxonomy enrichment specifically
The OCD-to-Wikipedia mapping is most valuable for the SubjectConcept gaps identified earlier — legal concepts, religious institutions, material culture. Wikipedia has articles on provocatio, lex Hortensia, the flamines, the Lupercalia, the toga praetexta. These articles have Wikidata QIDs. Those QIDs can become Chrystallum entity nodes with SubjectConcept memberships derived from their Wikipedia categories and OCD cross-references.
The enrichment is not about importing Wikipedia content — it is about using the Wikipedia/Wikidata layer as a discovery mechanism. Wikipedia tells you what concepts exist and are notable enough to have articles. Wikidata gives them stable identifiers. The OCD validates which of those concepts have genuine scholarly standing in the domain. Chrystallum then models the relationships between them at a depth neither Wikipedia nor Wikidata attempts.

The browser extension implications
This mapping is also what makes the browser extension genuinely useful rather than a demonstration. When a user is reading a Wikipedia article, the extension can immediately resolve the article to a Wikidata QID, look up that QID in Chrystallum, and surface the graph layer. No manual mapping by the user, no configuration. The Wikipedia-to-Wikidata connection is already built into Wikipedia's infrastructure — every Wikipedia article has a "Wikidata item" link in the sidebar. The extension just follows that link programmatically.
The OCD connection completes the authority chain. User reads Wikipedia on Sulla, extension shows the Chrystallum graph, user clicks on a claim, sees "this claim is supported by OCD 1949 p.847, Plutarch Life of Sulla, and three inscriptions from Trismegistos." That is the depth register made accessible, and the OCD is the authority anchor that distinguishes Chrystallum's claims from Wikipedia's own narrative.

The immediate practical step
Before building any of this, the OCD Wikidata item needs to be identified — the QID for the Oxford Classical Dictionary as a bibliographic source. From there, a SPARQL query returns every Wikidata entity already linked to the OCD. That list is the seed mapping and it is probably several hundred to a few thousand entries for the Roman Republic domain alone. The size of that seed set tells you how much of the mapping work Wikidata has already done for you.these are the download options dc.contributor.author: Denniston, J. D. Ed. dc.contributor.author: Nock, A. D. Ed. dc.contributor.author: Ross, W. D. Ed. dc.date.accessioned: 2015-09-22T15:19:28Z dc.date.available: 2015-09-22T15:19:28Z dc.date.digitalpublicationdate: 2014/02/20 dc.date.citation: 1949 dc.identifier.barcode: 07019990362691 dc.identifier.copyno: 1 dc.identifier.uri: http://www.new.dli.ernet.in/handle/2015/461705 dc.description.scannerno: 02 dc.description.scanningcentre: North Eastern States Libraries dc.description.main: 1 dc.description.tagged: 0 dc.description.totalpages: 988 dc.language.iso: English dc.publisher.digitalrepublisher: Digital Library Of India dc.publisher: Oxford University Press, Bombay dc.rights: In Public Domain dc.source.library: Birchandra State Central Library, Tripura dc.subject.classification: Generalities dc.subject.classification: Dictionary dc.subject.keywords: Aethra dc.subject.keywords: Basilica dc.subject.keywords: Roman dc.subject.keywords: Botany dc.subject.keywords: Brutus dc.title: The Oxford Classical Dictionary dc.type: Print - Paper dc.type: Dictionary dc.description.diskno: NE-DLI-TR-4557
Addeddate
2017-01-24 14:54:49
Identifier
in.ernet.dli.2015.461705
Identifier-ark
ark:/13960/t6k12hf39
Ocr
ABBYY FineReader 11.0
Ppi
600
Scanner
Internet Archive Python library 1.2.0.dev4
plus-circle Add Review
Reviews
There are no reviews yet. Be the first one to .
4,526 Views
12 Favorites
DOWNLOAD OPTIONS
download 1 file
ABBYY GZ download
DAISY
For users with print-disabilities
EPUB
download 1 file
FULL TEXT download
download 1 file
ITEM TILE download
download 1 file
PDF download
download 1 file
PDF WITH TEXT download
download 1 file
SINGLE PAGE PROCESSED JP2 TAR download
download 1 file
TORRENTShow more7:12 PMThe full text download is the right starting point. 7.3MB, plain text, already OCR'd by ABBYY FineReader at 600 PPI — that is high quality for a 1949 scan. Public domain, so no licensing issue.

What you actually have
988 pages, three editors — Cary as primary, with Denniston (Greek linguistics specialist), Nock (religion and mystery cults), and Ross (philosophy). That editorial division is itself useful metadata — entries on religious topics carry Nock's authority, entries on Greek linguistic matters carry Denniston's. When these become bibliography nodes, the editor attribution matters for confidence weighting in those domains.
The ABBYY OCR at 600 PPI on a clean dictionary layout should be reasonably accurate — probably 98%+ on standard Latin names, lower on Greek characters and diacriticals, which will need a cleanup pass. The dictionary format — headword in bold or caps, entry text, cross-references in small caps — is structured enough that a parser can extract the entry boundaries reliably.

The extraction pipeline, concretely
The full text download gives you a stream of OCR'd text. The immediate task is entry segmentation — identifying where each entry starts and ends. OCD entries follow a consistent pattern: headword in capitals, followed by entry text, ending before the next capitalized headword. Cross-references appear as small caps in the original, which the OCR will have rendered inconsistently — sometimes as ALL CAPS, sometimes as mixed case. That is the main parsing challenge.
A reasonable first pass:
python# Pattern: entry starts with a line beginning with 
# 2+ consecutive uppercase words (the headword)
# Entry text follows until next such pattern
# Cross-references identified by known entity names 
# appearing in the extracted entity list
The output of the first pass is a structured JSON — one object per entry, with fields for headword, entry text, cross_references extracted, page number, and raw_text. That JSON becomes the OCD corpus that agents query.
The second pass is the Wikidata alignment — for each headword, attempt a Wikidata entity lookup by label in the Roman Republic domain. Matches with high confidence get a QID stamped on the entry object. Uncertain matches get flagged for review. No match gets flagged as a potential taxonomy gap.

What to do with it now versus later
Now, before the SFA layer is ready: download the full text, run the entry segmentation, produce the headword list. That list alone — all OCD headwords, ~3,000-4,000 entries — is immediately useful for the SubjectConcept gap analysis. Cross-reference it against the 45 current SubjectConcepts and the 6,057 entities in the graph. The gaps are visible without any further processing.
The Wikidata alignment query runs in parallel — SPARQL against Wikidata for all items where P1343 points to the OCD. That returns the entities Wikidata already knows are in the OCD, with their QIDs. Merge with the headword list to get the seed mapping.
Later, when the SFA layer is further along: the entry texts become the grounding corpus. An SFA processing a claim about Sulla retrieves the OCD Sulla entry as context before reasoning. The entry text is not ingested as claims — it is reference material the agent reads, the same way a historian reads the OCD before writing. The claims the agent generates are attributed to the agent's reasoning, grounded against the OCD text, not extracted from it mechanically.

The one thing worth doing today
Download the full text file right now and do a quick character-level scan — how many entries roughly, what does the cross-reference markup look like in the OCR output, are Greek characters rendered as Latin transliterations or as garbled sequences. That five-minute inspection tells you how much parsing work the extraction pipeline requires. The difference between "cross-references appear as SMALL CAPS reliably" and "cross-references are indistinguishable from regular text" changes the pipeline complexity significantly.
The URL for the full text is directly accessible: https://archive.org/stream/in.ernet.dli.2015.461705/2015.461705.The-Oxford_djvu.txt
Want me to fetch a sample of it now so you can see the OCR quality before committing to the extraction work?yes7:13 PMSearched the webSearched the webFailed to fetch https://archive.org/stream/in.ernet.dli.2015.461705/2015.461705.The-Oxford_djvu.txtFailed to fetch https://archive.org/download/in.ernet.dli.2015.461705/2015.461705.The-Oxford_djvu.txtDoneThe direct download URL is blocked on my end — Archive.org's download paths require a session or referrer that I can't replicate from here. But you can pull the sample yourself in about 30 seconds and paste the first few hundred lines here.
In your browser, go directly to:
https://archive.org/download/in.ernet.dli.2015.461705/2015.461705.The-Oxford_djvu.txt
It will either open as text in the browser or download automatically. If it opens in the browser, copy the first 50-100 lines — from the front matter through the first few complete entries — and paste them here. That will tell us everything we need to know about OCR quality and entry structure before writing any parsing code.
Specifically looking for: how headwords appear (ALL CAPS, bold markers, standalone lines), how cross-references are marked in the OCR output, whether Greek terms are transliterated or garbled, and whether page numbers appear inline or as separate markers. Those four things determine the parsing approach.ABBREVIATIONS USED IN THE PRESENT WORK 


E. Rohde, Der grieckische Roman u. t. 

Vorl&rfer* (1914) 

RSmtach 
see Mommaen 
see Beloch 

Mitieilungen des Deutschen Arehdolog. 
Instituts, Rdmische Abteilung, 
1886- 

Bee Mommaen 

»• »» 
aee Cichonua 

W. H. Roacher, Ausfiihrliehepasted7:24 PMThis is excellent OCR quality. Better than expected for a 1949 scan. Let me assess what we actually have.

OCR quality assessment
The text is highly readable. Entry boundaries are clean — headwords appear in ALL CAPS followed by a dagger symbol (}) which is the OCR rendering of the original bold or small-caps formatting. That dagger is a reliable entry delimiter — it appears consistently on ABACUS}, ABARIS}, ABDERA}, and so on. This makes entry segmentation straightforward.
Cross-references appear as (q.v.) — quod vide, "which see." Every (q.v.) is a cross-reference to another OCD entry. That is a machine-readable relationship signal. ABARIS cross-references ARISTEAS and HYPERBOREAN via (q.v.). ACAMAS cross-references THESEUS and DEMOPHON. ACARNAN cross-references ALCMAEON. These are the edges of the OCD's own knowledge graph, already marked.
Author attribution appears at the end of each entry as initials — W. D. R. (Ross), W. K. C. G. (Guthrie), M. C. (Cary), H. J. R. (Rose), G. C. F. (Field). This is the editor attribution that lets agents know whose scholarly judgment a specific entry represents.
Greek characters have been transliterated cleanly — no garbled sequences. The OCR handled the diacriticals as Latin equivalents which is actually preferable for parsing.
The one noise pattern is the abbreviations section at the front — several pages of bibliography abbreviations before the actual dictionary entries begin. The entry ABACUS} marks the true start of the dictionary content.

What the entry structure gives you
Each entry parses to a clean object:
json{
  "headword": "ABDERA",
  "entry_text": "a Greek city in Thrace...",
  "cross_references": ["SICELS", "VILLANOVANS"],
  "author_initials": "M. C.",
  "author": "Cary, M.",
  "page_estimate": 1,
  "raw": "ABDERA} a Greek city in Thrace..."
}
The (q.v.) pattern extracts cross-references automatically. The author initials at the end of each entry are consistent enough to resolve against the known editor list. The headword before } is the canonical entity name.

What this means for Chrystallum specifically
The cross-reference graph is immediately extractable. Every (q.v.) pair — entry A references entry B — is a directed edge in a scholar-curated knowledge graph. That graph, extracted from the full text, gives you the OCD's own opinion about which entities are conceptually connected. For the salience problem discussed earlier — which connections matter most for a given entity — the OCD cross-reference graph is a powerful signal. If the OCD entry on Sulla cross-references Marius, the proscriptions, and the Social War via (q.v.), those are the connections that a senior classicist in 1949 considered essential for understanding Sulla.
The ABORIGINES entry is a good example of taxonomy enrichment. It references SICELS and VILLANOVANS — both are entity candidates for Chrystallum that may not currently exist as nodes. The entry text explains the relationship: Siculi were displaced by Aborigines before Rome. That is a PRECEDED_BY or DISPLACED_BY relationship between entities, with the OCD as the source attribution.
The ACADEMY entry shows how a concept entry works — not a person or place but an institution with a lifecycle: founded c.385 BC, dissolved AD 529. Temporal bounds, organizational continuity, named leaders. That is a rich SubjectConcept candidate with FOUNDED_BY, FOLLOWED_BY, and COLLAPSED edges waiting to be instantiated.

The parsing script, concretely
The entry delimiter } after a headword makes this simpler than typical OCR parsing:
pythonimport re

def parse_ocd_entries(text):
    # Split on headword pattern: ALL_CAPS word(s) followed by }
    pattern = r'([A-Z][A-Z\s,]+})'
    parts = re.split(pattern, text)
    
    entries = []
    for i in range(1, len(parts), 2):
        headword = parts[i].replace('}', '').strip()
        body = parts[i+1].strip() if i+1 < len(parts) else ''
        
        # Extract cross-references
        xrefs = re.findall(r'\(q\.v\.\)', body)
        # Get the word immediately before each (q.v.)
        xref_terms = re.findall(
            r'(\w[\w\s]+?)\s*\(q\.v\.\)', body
        )
        
        # Extract author initials (last token pattern X. X. X.)
        author = re.search(
            r'([A-Z]\.\s*){2,}$', body.strip()
        )
        
        entries.append({
            'headword': headword,
            'body': body,
            'cross_references': xref_terms,
            'author_initials': author.group(0).strip() 
                               if author else None
        })
    
    return entries
This is a first pass — it will need tuning for multi-word headwords and entries that span page breaks — but the } delimiter makes the core segmentation reliable.

Recommended next step
Download the full text, run the entry segmentation, and produce two outputs: the headword list as a flat file, and the cross-reference graph as an edge list. The headword list goes into the SubjectConcept gap analysis immediately. The cross-reference graph becomes a salience input for the persona layer. Both are usable before any further pipeline work.
The full text is 7.3MB — probably 3,500 to 4,500 entries given 988 pages at roughly 2-3 entries per page on average, with longer entries reducing that count. The Roman Republic-relevant entries will be a subset — probably 800-1,200 — which is the working corpus for Chrystallum's domain.
Want me to draft the full parsing script now, or start with the Wikidata SPARQL query to find which OCD entries already have QID mappings?lets check mappings\7:26 PMSearched the webSearched the webWikidata Oxford Classical Dictionary P1343 QID source item10 resultsProperty talk:P1343 - Wikidatawww.wikidata.orgWikidata - Wikipediaen.wikipedia.orgThe Oxford classical dictionary : Free Download, Borrow, and Streaming : Internet Archivearchive.orgdescribed by source - Wikidatawww.wikidata.orgOxford Classical Dictionary (4th rev. ed.) - Wikidatawww.wikidata.orgOxford Classical Dictionary (3rd rev. ed.) - Wikidatawww.wikidata.orgQuerying Wikidata with SPARQL – Loretta C. Duckworth Scholars Studiosites.temple.eduOxford Classical Dictionary ID - Wikidatawww.wikidata.orgOxford Classical Dictionary - Wikidatawww.wikidata.orgThe Oxford Classical Dictionaryia902909.us.archive.orgDoneGood. The search has surfaced exactly what we need. Several important findings already visible.Searched the webSearched the webFailed to fetch https://www.wikidata.org/wiki/Q430486Failed to fetch https://www.wikidata.org/wiki/Property:P9106DoneWikidata's wiki interface is blocked, but the search results gave us everything we need without fetching. Here is what we have.

The QIDs are identified
The OCD exists in Wikidata as multiple items — one per edition:

Q430486 — Oxford Classical Dictionary (the general work item)
Q20078571 — OCD 3rd revised edition (1996)
Q69525831 — OCD 4th revised edition (online)

The 1949 edition you have downloaded likely maps to Q430486 or has its own item. This needs verification but the general work QID is the anchor.
P9106 is the dedicated property — "Oxford Classical Dictionary ID" — described as "identifier for a topic in the Oxford Classical Dictionary online." This is the 4th edition online identifier, not the 1949 print edition. But it means Wikidata has already built infrastructure for OCD-to-entity mapping. Any Wikidata entity with P9106 set has an explicit OCD online cross-reference.

What this means practically
There are two separate mapping paths:
Path 1 — P9106 (OCD online ID): Entities where Wikidata editors have explicitly linked to the OCD online. A SPARQL query on P9106 returns every entity already mapped. This is likely thousands of entries — the OCD online is the 4th edition and widely referenced. These entities have confirmed OCD coverage and stable identifiers.
Path 2 — P1343 with Q430486 (described by source): Entities where editors have added the OCD as a bibliography source using P1343. This is a softer link — it says "this entity is described in the OCD" without specifying which entry.
The SPARQL query to run against the Wikidata Query Service at query.wikidata.org:
sparqlSELECT ?item ?itemLabel ?ocdId WHERE {
  ?item wdt:P9106 ?ocdId .
  SERVICE wikibase:label { 
    bd:serviceParam wikibase:language "en". 
  }
}
LIMIT 1000
And separately for P1343:
sparqlSELECT ?item ?itemLabel WHERE {
  ?item wdt:P1343 wd:Q430486 .
  SERVICE wikibase:label { 
    bd:serviceParam wikibase:language "en". 
  }
}
These two queries together give you the seed mapping — every entity Wikidata already knows is in the OCD. The P9106 results will be the richer set because they include the OCD entry identifier, not just a loose bibliographic reference.

The critical insight
P9106 is the 4th edition online identifier, not the 1949 edition. The 4th edition is substantially revised — many entries reorganized, some merged, some split, new scholarship incorporated. The headwords in your 1949 text will not always match the P9106 identifiers directly. But the entity-level mapping is still valid — if Sulla has P9106 set in Wikidata, Sulla is in the OCD across editions, and the 1949 entry text for SULLA is the historical scholarly judgment on the same entity.
This means the mapping strategy is: use P9106 to identify which entities have OCD coverage confirmed by Wikidata, then match those entities to your parsed 1949 headwords by label normalization. Where the headword matches an entity with P9106, you have a three-way link: 1949 entry text → Wikidata QID → OCD online ID. That three-way link is what connects the historical scholarship to the modern linked data infrastructure.

Immediate action
Run both SPARQL queries at https://query.wikidata.org — the interface is public and requires no login. The P9106 query especially — paste the results here and we will know immediately how much of the mapping work is already done and what the domain coverage looks like for Roman Republic entities specifically.For the Roman Republic specifically, the most useful online academic resources cluster into a few categories: prosopography, economy, numismatics, text corpora, and digital projects. [digitalromanheritage](https://digitalromanheritage.com/projects/)

## Core prosopography and people

- **Digitalpastedwill do. what about7:41 PMYou cut off — "what about" didn't complete. What were you going to ask?the paste7:42 PMThe document you pasted is a research compilation of digital Roman Republic resources. This is significant — several of these are either direct federation candidates or mapping resources that change the OCD-to-Wikidata picture substantially.

The immediate standout: DPRR
The Digital Prosopography of the Roman Republic at King's College London is the most important item on this list for Chrystallum. It is exactly what Trismegistos and LGPN are for the broader ancient world, but scoped specifically to Republican elite prosopography — offices, careers, status, family relations. If DPRR has an API or RDF export, it becomes a Phase 2 federation candidate that is more domain-precise than either Trismegistos or LGPN for the Republican period.
The critical question is whether DPRR entities have Wikidata QIDs or can be aligned to them. If DPRR uses its own identifiers and Wikidata has P1343 or a dedicated property linking to DPRR records, the three-way mapping — OCD entry → Wikidata QID → DPRR person record — is the prosopographic backbone of the entire graph. That is a more powerful foundation than anything currently in the architecture.

The romanrepublic.ac.uk RDF dataset
This is directly relevant to the OCD mapping question you were asking about. The "Shape of Roman History" project exposes Republican persons and relationships in RDF — which means it already has structured linked data for Republican entities, likely with Wikidata alignment. This is not just a reference source. It is a potential direct import into Chrystallum's graph layer. RDF triples describing Republican persons and their relationships are exactly the edge-building layer that is currently missing.
Before building that layer from scratch, the romanrepublic.ac.uk dataset should be examined. If it covers even 30% of the entities Chrystallum needs and is already in RDF, that is months of work already done by someone else under an open license.

Project Mercury and coin hoards
This is the economic and numismatic layer — coin hoard findspots with Nomisma.org URIs and GeoNames coordinates. For Chrystallum this is a spatial federation candidate. Coin hoards are archaeological evidence of economic activity, military movements, and political disruption. A hoard found in a particular province dated to the Sullan period is not just numismatic data — it is evidence of the political economy of that moment. The GeoNames and Nomisma URIs mean these entities are already in the linked data universe and alignable to Pleiades place entities via geographic coordinates.

OxRep
The Oxford Roman Economy Project is the quantitative layer — population, trade, prices, economic performance. For the browser extension use case and the reader persona, economic data grounds the narrative. When a user is reading about the Gracchan land reforms, the graph should be able to surface "this is the economic context — land concentration, slave labor displacement, declining small farmer population" as quantified claims, not just narrative assertions. OxRep is the source for those claims.

Perseus Digital Library
Already relevant to the OCD mapping question. Perseus has morphologically tagged Latin and Greek texts with some linked data hooks. The primary sources that OCD entries cite — Livy, Polybius, Plutarch, Cicero — are in Perseus with stable URIs. That means OCD citations like "Hdt. 4.36" or "Apollod. 3.91-3" are resolvable to specific Perseus passages. The citation becomes a navigable link from the graph to the primary source text. That is the attestation chain made clickable — the reader persona can follow a claim from the graph to the OCD entry to the primary source passage in Perseus.

What this does to the federation architecture
The current federation list has 13 federations, with Trismegistos, LGPN, and Pleiades as the primary Phase 2 targets. This document suggests several additions worth evaluating:
DPRR is the highest priority addition — more domain-specific than anything currently in the federation list for Republican elite prosopography. If it has a queryable endpoint it should be added as a Phase 2 federation.
Romanrepublic.ac.uk RDF is worth examining as a potential direct import rather than a federation — if the dataset is openly licensed and covers Republican persons and relationships, importing it is faster than federating against it.
Project Mercury coin hoards adds the spatial-economic layer that the current federations do not cover.
Perseus adds primary source text resolution — not entity data but citation resolution, which is different. It is the text layer beneath the prosopographic layer.

Back to the OCD mapping question
The DPRR and romanrepublic.ac.uk RDF almost certainly have OCD cross-references built in — these are academic projects that cite the OCD as a standard reference. If DPRR person records include OCD headword references, that is the mapping you were looking for, already done at the prosopographic level by domain specialists. Before running the Wikidata SPARQL query, it is worth checking whether DPRR's data model includes OCD citations. If it does, the mapping path is OCD headword → DPRR person ID → Wikidata QID, which is more authoritative than a label-matching approach.
What do you want to pull on first — DPRR specifically, the romanrepublic.ac.uk RDF, or the Wikidata P9106 SPARQL query?u choose7:49 PMDPRR first. It is the highest value target and if it has Wikidata alignment it changes the federation architecture significantly. Let me look at what it actually exposes.Fetched: Digital Prosopography of the Roman Republic <small>DPRR</small> | King's Digital LabFetched: Digital Prosopography of the Roman Republic <small>DPRR</small> | King's Digital LabDigital Prosopography of the Roman Republic <small>DPRR</small> | King's Digital Labkdl.kcl.ac.ukDonePost-project status noted — the live database is at romanrepublic.ac.uk. Let me go directly there and check the data model and API.Searched the webSearched the webDigital Prosopography of the Roman Republicromanrepublic.ac.ukFailed to fetch https://romanrepublic.ac.uk/rdf/doc/Failed to fetch https://romanrepublic.ac.uk/technical-overview/Failed to fetch https://romanrepublic.ac.uk/person/DoneThe site itself is blocking fetch. Let me check the GitHub repository directly — the code will tell us the data model and API endpoints.Fetched: GitHub - kingsdigitallab/dprr-django: Code repository for the Digital Prosopography of the Roman Republic projectFetched: GitHub - kingsdigitallab/dprr-django: Code repository for the Digital Prosopography of the Roman Republic projectGitHub - kingsdigitallab/dprr-django: Code repository for the Digital Prosopography of the Roman Republic projectgithub.comDoneMIT license — that is the critical finding. Let me look at the data directory and models.Searched the webSearched the webFailed to fetch https://github.com/kingsdigitallab/dprr-django/tree/master/dataFailed to fetch https://raw.githubusercontent.com/kingsdigitallab/dprr-django/master/README.mdromanrepublic.ac.uk DPRR RDF API endpoint person data model linked open data10 resultsTechnical Overview | DPRRromanrepublic.ac.ukHome | DPRRromanrepublic.ac.ukDPRR RDF: Documentation: Using the Serverromanrepublic.ac.ukDPRR RDF: Documentationromanrepublic.ac.ukGitHub - kingsdigitallab/dprr-django: Code repository for the Digital Prosopography of the Roman Republic projectgithub.comDHQ: Digital Humanities Quarterly: A Prosopography as Linked Open Data: Some Implications from DPRRdhq.digitalhumanities.orgDPRR RDF: Documentation: Why RDF?romanrepublic.ac.ukDPRR RDF: Documentation: the Ontologyromanrepublic.ac.ukA Prosopography as Linked Open Data: Some Implications from DPRR - King's College Londonkclpure.kcl.ac.ukPoMS RDF: Documentationpoms.ac.ukDoneThis is better than expected. Let me summarize what we now know clearly.

DPRR is almost exactly what Chrystallum needs as a federation
The data model maps directly onto Chrystallum's architecture in ways that are not coincidental — they are solving the same problem. Four core entities: Person, PostAssertion, StatusAssertion, RelationshipAssertion. Every assertion is linked to a secondary source via hasSecondarySource. The ontology is OWL-based at http://romanrepublic.ac.uk/rdf/ontology#. The SPARQL endpoint is live at http://romanrepublic.ac.uk/rdf/endpoint/.
Cicero's URI is http://romanrepublic.ac.uk/rdf/entity/Person/2072. That is the pattern — every person has a stable URI that resolves to RDF triples.
The secondary sources in DPRR are exactly the right sources — Broughton's Magistrates of the Roman Republic, Rüpke's Fasti Sacerdotum, Zmeskal's Adfinitas, Pina Polo's repulsae. These are the same authoritative sources Chrystallum's SFAs would consult. DPRR has already done the structured data extraction from them.
MIT license confirmed from the GitHub repository. Freely usable.

What DPRR does not have — and this is the key gap
The ontology documentation explicitly states: "DPRR's research team found that there was insufficient time to resolve these references to actual Primary Source objects." So DPRR has secondary source attribution but not primary source grounding. It links assertions to Broughton, not to Livy or inscriptions directly.
This is precisely the gap Chrystallum fills. DPRR says "Broughton says Caesar was consul in 59 BC." Chrystallum adds "Broughton derives this from Livy, which is corroborated by CIL inscription TM12345, confidence 0.95." DPRR is the structured secondary layer. Chrystallum is the epistemological layer on top of it.

The SPARQL endpoint is the federation mechanism
The endpoint at http://romanrepublic.ac.uk/rdf/endpoint/ accepts standard SPARQL. The Phase 2 federation client for DPRR is a SPARQL query client, not a custom API. A query to get all persons with their offices and relationships:
sparqlPREFIX vocab: <http://romanrepublic.ac.uk/rdf/ontology#>

SELECT ?person ?praenomen ?nomen ?cognomen ?office ?year WHERE {
  ?person a vocab:Person ;
          vocab:praenomen ?praenomen ;
          vocab:nomen ?nomen .
  OPTIONAL { ?person vocab:cognomen ?cognomen }
  OPTIONAL {
    ?post a vocab:PostAssertion ;
          vocab:isAboutPerson ?person ;
          vocab:hasOffice ?office ;
          vocab:inYear ?year .
  }
}
LIMIT 100
And for relationships:
sparqlPREFIX vocab: <http://romanrepublic.ac.uk/rdf/ontology#>

SELECT ?person1 ?person2 ?relType WHERE {
  ?rel a vocab:RelationshipAssertion ;
       vocab:isAboutPerson ?person1 ;
       vocab:hasRelatedPerson ?person2 ;
       vocab:hasRelationship ?relType .
}
LIMIT 100

The OCD mapping question answered indirectly
DPRR's secondary sources include references that would allow OCD headword alignment. Broughton's MRR and Rüpke's Fasti are the same authoritative sources the OCD 1949 edition cites. A person in DPRR with a Broughton citation and a person in the OCD with the same Broughton citation are almost certainly the same entity. The alignment path is: DPRR person URI → Broughton reference → OCD headword cross-reference → Wikidata QID via P9106.

Immediate next steps
Two things worth doing right now. First, test the SPARQL endpoint is live by running a simple count query — this is a post-project site and KDL maintains it under SLA, but worth confirming before building a federation client against it. Second, look at whether DPRR person URIs are already in Wikidata as external identifiers — if Wikidata items for Republican persons carry DPRR URIs, the Wikidata QID alignment is already done.
Want to test the endpoint and check the Wikidata alignment simultaneously?pin that. how does all this affect the timing of running foward links8:18 PMGood question to ask now before committing to the forward SPARQL work.

The short answer
DPRR changes the priority order significantly. The LGPN forward SPARQL for Q899409 (Roman families) was designed to solve a specific problem — persons with family attestations don't backlink to the family anchor, so the harvest misses them. DPRR solves that same problem more completely and with better data than a forward SPARQL hack against Wikidata would.

What the forward links script was going to do
The design was: query Wikidata for persons with P1838 (LGPN ID) and P53 (family/gens), filter by date range, post-filter by SubjectConcept hierarchy. The output would be Republican persons with LGPN attestation and family membership, imported as temporal_scoped entities.
The problem with that approach: Wikidata's P53 coverage for Republican gens membership is sparse and inconsistent. Many persons in Wikidata don't have P53 set even when their gens affiliation is known. The harvest would miss a large fraction of the persons you actually want.

What DPRR gives you instead
DPRR has RelationshipAssertion with hasRelationship typed as family relationships — brother of, son of, father of — connecting Person to Person. It has PostAssertion linking persons to offices with years. It has StatusAssertion for senatorial and equestrian class. All sourced from Broughton, Zmeskal, and Rüpke.
For Q899409 (Roman families) the DPRR query is direct and complete:
sparqlPREFIX vocab: <http://romanrepublic.ac.uk/rdf/ontology#>

SELECT ?person ?nomen ?cognomen ?relType ?relatedPerson WHERE {
  ?person a vocab:Person ;
          vocab:nomen ?nomen .
  OPTIONAL { ?person vocab:cognomen ?cognomen }
  ?rel a vocab:RelationshipAssertion ;
       vocab:isAboutPerson ?person ;
       vocab:hasRelatedPerson ?relatedPerson ;
       vocab:hasRelationship ?relType .
  FILTER(CONTAINS(STR(?nomen), "Iulius") || 
         CONTAINS(STR(?nomen), "Cornelius") ||
         CONTAINS(STR(?nomen), "Sempronius"))
}
That returns family relationship data for specific gentes directly, sourced from Zmeskal's Adfinitas — the dedicated reference work on Roman family connections. That is categorically better than inferring family membership from Wikidata P53.

The timing implication
The forward SPARQL script should be deprioritized or redesigned. There are three possible positions:
Option 1 — Defer entirely. Don't build the LGPN forward SPARQL at all. Test the DPRR endpoint, build the DPRR federation client instead, and get family/gens data from DPRR. The forward SPARQL was a workaround for a data gap that DPRR fills properly.
Option 2 — Narrow scope. Keep the LGPN forward SPARQL but scope it only to what DPRR doesn't cover — non-elite persons, freedmen, women, provincials with LGPN attestation who are outside DPRR's elite focus. DPRR explicitly covers upper strata society. The long tail of non-elite Republican persons is still a gap DPRR won't fill.
Option 3 — Sequence them. Build the DPRR federation client first, import what it covers, then assess what gaps remain in Q899409 before deciding whether the forward SPARQL is worth building at all.

The right answer is Option 3
DPRR's scope is elite Roman Republican society — exactly Q899409's domain. After importing DPRR data for family relationships, run the SubjectConcept gap analysis again. If Q899409 cluster coverage is still thin on specific gentes or time periods, then build the forward SPARQL to fill those specific gaps. Build it to the actual gap, not to a theoretical gap that may not exist after DPRR import.

What this does to the kanban
The forward SPARQL item moves from "next" to "conditional pending DPRR assessment." The DPRR federation client moves up to immediate priority alongside the Wikidata P9106 alignment check. The OCD parsing script stays on the queue but behind DPRR — the OCD enriches what DPRR provides, it doesn't replace it.
The sequence becomes: test DPRR endpoint → build DPRR federation client → import persons, offices, relationships → assess Q899409 coverage → decide on forward SPARQL scope → OCD parsing and alignment → full 61-anchor production re-harvest with class gating.
That is a better-sequenced pipeline than what was on the kanban before this conversation.