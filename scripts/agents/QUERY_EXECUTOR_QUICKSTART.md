# Query Executor Agent - Quickstart Guide

## Overview
The Query Executor Agent is a ChatGPT-powered Neo4j query agent that can:
- Execute natural language queries against the Chrystallum knowledge graph
- Discover Neo4j schema dynamically
- Generate and execute Cypher queries
- Submit claims to the knowledge graph with automatic promotion

## Prerequisites

### 1. Environment Setup
Ensure you have Python 3.8+ and the required packages installed:

```bash
cd scripts/agents
pip install -r requirements.txt
```

Required package versions:
- neo4j>=5.0.0
- openai>=0.27.0

### 2. Environment Variables
Set these before running the agent:

**On Windows PowerShell:**
```powershell
$env:NEO4J_PASSWORD = "your_neo4j_password"
$env:OPENAI_API_KEY = "your_openai_api_key"
$env:NEO4J_URI = "bolt://localhost:7687"  # or neo4j+s://your_aura_instance
$env:NEO4J_USERNAME = "neo4j"
$env:NEO4J_DATABASE = "neo4j"
```

**On Windows Command Prompt:**
```cmd
set NEO4J_PASSWORD=your_neo4j_password
set OPENAI_API_KEY=your_openai_api_key
set NEO4J_URI=bolt://localhost:7687
set NEO4J_USERNAME=neo4j
set NEO4J_DATABASE=neo4j
```

**On macOS/Linux:**
```bash
export NEO4J_PASSWORD="your_neo4j_password"
export OPENAI_API_KEY="your_openai_api_key"
export NEO4J_URI="bolt://localhost:7687"
export NEO4J_USERNAME="neo4j"
export NEO4J_DATABASE="neo4j"
```

## Usage Modes

### Mode 1: Run Predefined Query Tests
Tests the agent's ability to discover schema and generate Cypher queries:

```bash
python query_executor_agent_test.py test
```

**Expected Output:**
- Schema discovery (labels and relationship types)
- 3-5 predefined queries executed
- Results displayed in formatted output
- Sample queries: "Show all Events", "Find Romans", etc.

**Success Criteria:**
- No connection errors
- Valid Cypher generated by ChatGPT
- Query results returned
- No database errors

---

### Mode 2: Run Claim Submission Tests
Tests the full claim ingestion and promotion workflow:

```bash
python query_executor_agent_test.py claims
```

**What It Tests:**
1. **Low Confidence Claim (0.75):**
   - Creates claim node
   - Links to source and target entities
   - Creates intermediary nodes (Context, Analysis, Facet)
   - Status: "created" (not promoted)

2. **High Confidence Claim (0.95):**
   - Creates claim node
   - Attempts promotion (if confidence >= 0.90)
   - Creates canonical relationship if promoted
   - Creates SUPPORTED_BY traceability edges
   - Status: "promoted" (if prerequisites met)

**Expected Output:**
```
[Test 1] Low confidence claim (proposed)
status: created
claim_id: claim_<12hex>
cipher: <64-char SHA256 hash>
promoted: false

[Test 2] High confidence claim (should promote if prerequisites met)
status: promoted
claim_id: claim_<12hex>
cipher: <64-char SHA256 hash>
promoted: true
```

**Success Criteria:**
- No exceptions
- Claim nodes created in Neo4j
- Low confidence: promoted=false
- High confidence: promoted=true (if context nodes exist)
- SUPPORTED_BY relationships visible in graph

---

## Facet Model (Normalized)

- Facet keys are **lowercase** and sourced from `Facets/facet_registry_master.json`.
- There are **17 facets**, including `communication`.
- Model: **one claim per facet** per entity-relationship; use `facet: "NA"` when evidence is insufficient.

Example:
```python
executor.submit_claim(
    entity_id="evt_battle_of_actium_q193304",
    relationship_type="OCCURRED_DURING",
    target_id="prd_roman_republic_q17167",
    confidence=0.95,
    label="Battle of Actium in Roman Republic",
    facet="military",
    claim_signature={
        "qid": "Q17167",
        "pvalues": ["P31", "P361"],
        "values": {"P31": "Q3024240", "P361": "Q17167"}
    }
)
```

**Strict signature requirement:**
- `claim_signature` must be a dict with `qid`, `pvalues` (list of P-IDs), and `values` (dict keyed by P-IDs).
- `claim_signature.qid` must match `subject_qid`.

Communication facet prompt:
```
How and when was this communicated? Propaganda, ceremony, messaging, narrative framing?
If no evidence -> facet: "NA"
```

---

## Authority Provenance Tracking

The claim ingestion pipeline supports capturing authority/source information for all claims. This enables upstream traceability to external data sources like Wikidata, LCSH, or other authority systems.

### Authority Fields

When submitting a claim, you can specify:
- **`authority_source`** (string): The authority system name (e.g., `"wikidata"`, `"lcsh"`, `"freebase"`)
- **`authority_ids`** (string, dict, or list): Authority identifiers from the source system

### Examples

**Example 1: Wikidata Authority with QID**
```python
executor.submit_claim(
    entity_id="evt_battle_of_actium_q193304",
    relationship_type="OCCURRED_DURING",
    target_id="prd_roman_republic_q17167",
    confidence=0.95,
    label="Battle of Actium in Roman Republic",
    subject_qid="Q17167",
    facet="military",
    authority_source="wikidata",
    authority_ids={"Q17167": "P31", "Q193304": "P580"},
    claim_signature={
        "qid": "Q17167",
        "pvalues": ["P31", "P361"],
        "values": {"P31": "Q3024240", "P361": "Q17167"}
    }
)
```

**Example 2: LCSH Authority with Subject Headings**
```python
executor.submit_claim(
    entity_id="subj_history_of_rome",
    relationship_type="CLASSIFIED_BY",
    target_id="subj_military_history",
    confidence=0.90,
    label="Roman military history classified as military history",
    authority_source="lcsh",
    authority_ids={"sh85110847": "History of Rome", "sh85088321": "Military history"},
    facet="communication"
)
```

**Example 3: Authority as Simple String**
```python
executor.submit_claim(
    entity_id="plc_rome_q220",
    relationship_type="IS_CAPITAL_OF",
    target_id="prd_italian_kingdom_q172579",
    confidence=0.98,
    label="Rome is the capital of Italy",
    authority_source="wikidata",
    authority_ids="Q220",  # Simple string ID
    facet="geographic"
)
```

### Storage

Authority information is persisted on:
- **Claim nodes**: `authority_source`, `authority_ids` properties
- **RetrievalContext nodes**: `authority_source`, `authority_ids` properties

This allows queries to trace claims back to their original authority sources.

---

## Fischer Fallacy Flagging

**All fallacies are detected and flagged.** Promotion decisions are based **purely on scientific metrics** (confidence + posterior), never on fallacy presence. Flag intensity guides downstream human review prioritization.

### Promotion Rule (Universal)

```
IF confidence >= 0.90 AND posterior >= 0.90
   THEN promoted = true
   (fallacies flagged as 'none', 'low', or 'high' for downstream triage)
ELSE
   promoted = false
```

### Fallacy Flag Intensity

All detected fallacies are categorized by risk profile to guide downstream review:

**Flag Intensity: 'high'** (Interpretive Claims)
- Applies to claims with reasoning about motivations, causation, meaning, or narrative
- **Warrant closer human review** before acceptance into knowledge graph
- Claim Types: `causal`, `interpretive`, `motivational`, `narrative`
- Facets: `political`, `diplomatic`, `religious`, `social`, `communication`, `intellectual`, `military`, `cultural`, `economic`

**Flag Intensity: 'low'** (Descriptive Claims)
- Applies to purely factual or structural claims (what, where, when, who, classification)
- **Lower concern**; promotable with normal review
- Claim Types: `temporal`, `locational`, `taxonomic`, `identity`
- Facets: `geographic`, `environmental`, `archaeological`, `scientific`, `technological`, `demographic`, `linguistic`, `artistic`

**Flag Intensity: 'none'** (No Fallacies Detected)
- Claim promoted normally with no fallacy concerns

### Examples

**Example 1: Promoted with High-Intensity Flag**
```python
executor.submit_claim(
    entity_id="fig_caesar_q1",
    relationship_type="MOTIVATED_BY",
    target_id="evt_gallic_wars_q123",
    confidence=0.92,
    label="Caesar wanted to conquer Gaul for political ambitions",
    facet="political",
    claim_type="motivational",
    reasoning_notes="Caesar's intentions were to gain military prestige for political power"
)
# Result: PROMOTED = true (confidence + posterior ≥ 0.90)
# Fallacy: "post_hoc_causation" detected
# Flag: fallacy_flag_intensity = "high" → triggers human review before acceptance
```

**Example 2: Promoted with Low-Intensity Flag**
```python
executor.submit_claim(
    entity_id="evt_actium_q193304",
    relationship_type="LOCATED_IN",
    target_id="plc_actium_q41747",
    confidence=0.95,
    label="Battle of Actium took place at Actium",
    facet="geographic",
    claim_type="locational",
    reasoning_notes="Historical sources confirm battle location"
)
# Result: PROMOTED = true (confidence + posterior ≥ 0.90)
# Fallacy: None detected OR detected but low concern
# Flag: fallacy_flag_intensity = "low" or "none" → normal review flow
```

### Rationale

- **Metrics-based promotion:** Confidence + posterior probability are scientific; fallacy heuristics are imperfect and should not block valid claims
- **Preserved audit trail:** All fallacies are logged and flagged in response; downstream systems and humans can decide what action to take
- **Risk-stratified review:** Flag intensity enables humans to prioritize review effort on high-risk claims (interpretive reasoning) vs. low-risk claims (factual assertions)

---

## Launch Training Workflow (Seed QID)

On launch, the agent runs a one-time training pass for `Q17167` (Roman Republic):
- Full statements export + datatype profiling
- Backlink harvest (expanded caps) + backlink profiling
- Proposal generation capped at **1000 nodes**
- Stop for human review before any Neo4j ingestion

Optional: run a second pass with adjusted limits after review.

**Training Constraints (Required):**
- Record run metadata in the proposal: mode, caps, and whether trimming occurred.
- If the proposal exceeds 1000 nodes, document the trimming rule used (drop lowest-priority nodes).
- Log `class_allowlist_mode` and any overrides used during harvest.
- If any budget caps are hit, mark the proposal as partial and recommend a second pass.

---

### Mode 3: Interactive Query Mode
Manual testing with a REPL interface:

```bash
python query_executor_agent_test.py interactive
```

**Interactive Commands:**
```
> Show all humans in the graph
> Find events in 31 BCE
> What subjects are related to Q17167?
> exit
```

Type natural language queries. Agent will:
1. Discover schema if not already cached
2. Display current schema
3. Generate Cypher for your query
4. Execute and display results
5. Loop until "exit"

---

### Mode 4: Single Query Mode
Execute a single query without entering REPL:

```bash
python query_executor_agent_test.py "Show all Events in 31 BCE"
```

The agent will:
1. Generate Cypher for your query
2. Execute it
3. Display results
4. Exit

---

### Mode 5: Default Mode
If no arguments provided, runs basic tests:

```bash
python query_executor_agent_test.py
```

Equivalent to `python query_executor_agent_test.py test`

## Claim Submission Programmatically

### In Python Code:

```python
from query_executor_agent_test import ChromatogramQueryExecutor

# Initialize agent
executor = ChromatogramQueryExecutor()

# Submit a claim
result = executor.submit_claim(
    entity_id="evt_battle_of_actium_q193304",
    relationship_type="OCCURRED_DURING",
    target_id="prd_roman_republic_q17167",
    confidence=0.95,
    label="Battle of Actium in Roman Republic",
    subject_qid="Q17167",
    reasoning_notes="Agent observed historical sources confirming event timing",
    facet="military"
)

# Process result
if result["status"] == "promoted":
    print(f"Claim promoted! ID: {result['claim_id']}")
elif result["status"] == "created":
    print(f"Claim created (not promoted). ID: {result['claim_id']}")
else:
    print(f"Error: {result['error']}")

executor.close()
```

### Return Value:
```python
{
    "status": "created" | "promoted" | "error",
    "claim_id": "claim_<entity>_<relationship>_<target>",
    "cipher": "<SHA256 hash>",
    "promoted": True | False,
    "error": None | error_message
}
```

## Architecture

### Agent Components:
1. **Schema Discovery** - CALL db.labels() and CALL db.relationshipTypes()
2. **Query Generation** - ChatGPT translates NLQ to Cypher
3. **Query Execution** - Neo4j driver executes Cypher
4. **Claim Submission** - ClaimIngestionPipeline validates, creates, and promotes

### Claim Flow:
```
submit_claim()
    ↓
ClaimIngestionPipeline.ingest_claim()
    ↓
Validate entity/field existence
    ↓
Generate deterministic claim_id (MD5-based)
    ↓
Calculate SHA256 cipher for integrity
    ↓
Create Claim node with properties
    ├── Create RetrievalContext + USED_CONTEXT link
    ├── Create AnalysisRun + HAS_ANALYSIS_RUN link
    ├── Create FacetAssessment + HAS_FACET_ASSESSMENT link
    └── Link claim to source + target entities (ASSERTS)
    ↓
if confidence >= 0.90 and prerequisites met:
    ├── Update claim status to 'validated'
    ├── Create canonical relationship (entity -[type]-> target)
    ├── Link both entities back to claim (SUPPORTED_BY)
    └── Set promoted=true
    ↓
Return result dict
```

## Troubleshooting

### "NEO4J_PASSWORD not set"
**Solution:** Set the environment variable before running:
```powershell
$env:NEO4J_PASSWORD = "your_password"
```

### "OPENAI_API_KEY not set"
**Solution:** Set the environment variable before running:
```powershell
$env:OPENAI_API_KEY = "your_api_key"
```

### "Failed to authenticate to server"
**Possible Causes:**
- Incorrect password
- Neo4j service not running
- Wrong URI (check NEO4J_URI environment variable)

**Solution:**
```powershell
# Test connection manually
python -c "
from neo4j import GraphDatabase
driver = GraphDatabase.driver('bolt://localhost:7687', auth=('neo4j', 'your_password'))
with driver.session() as session:
    result = session.run('RETURN 1')
    print('Connection successful')
driver.close()
"
```

### "Failed to establish a connection"
**Causes:**
- Neo4j not running
- Wrong host/port in NEO4J_URI
- Network connectivity issue

**Solution:**
```powershell
# Check if Neo4j is running (standalone)
netstat -an | findstr 7687

# Or check Neo4j desktop app is running
# Try connecting to Aura instance if local doesn't work
$env:NEO4J_URI = "neo4j+s://your_aura_instance"
```

### ChatGPT returns invalid Cypher
**Causes:**
- API rate limit hit
- Invalid schema labels in graph
- Complex query that doesn't map to graph structure

**Solution:**
- Check ChatGPT output in console
- Simplify query
- Review schema output from agent
- Check Neo4j logs for syntax errors

### Claims not promoting
**Possible Causes:**
- Confidence < 0.90
- Missing RetrievalContext node
- Missing AnalysisRun node
- Target entity doesn't exist

**Solution:**
- Check test_claim_submission() output for error details
- Verify target entities exist: `MATCH (n) WHERE n.id = "your_entity_id" RETURN n`
- Check pipeline logs for validation failures

## Next Steps

1. **Test Basic Queries:**
   ```bash
   python query_executor_agent_test.py test
   ```

2. **Test Claim Submission:**
   ```bash
   python query_executor_agent_test.py claims
   ```

3. **Manual Exploration:**
   ```bash
   python query_executor_agent_test.py interactive
   ```

4. **Review Claim Nodes in Neo4j:**
   ```cypher
   MATCH (c:Claim) RETURN c LIMIT 10
   ```

5. **Check Promoted Claims:**
   ```cypher
   MATCH (c:Claim {promoted: true}) RETURN c
   ```

## API Reference

### ChromatogramQueryExecutor Methods

#### `__init__()`
Initializes agent, connects to Neo4j, discovers schema, creates pipeline.

#### `query(user_query: str) -> str`
Executes natural language query against graph.
- Generates Cypher using ChatGPT
- Executes against Neo4j
- Returns formatted results

#### `submit_claim(entity_id, relationship_type, target_id, confidence, label, subject_qid, retrieval_source=None, reasoning_notes=None, facet=None) -> Dict`
Submits claim to graph.
- Validates all parameters
- Creates claim with intermediary nodes
- Promotes if confidence >= 0.90
- Returns status dict

#### `close()`
Closes Neo4j connection. Call this before exiting.

#### `interactive_session()`
Starts REPL for manual query testing.

## Performance Notes

- **Schema discovery:** ~200ms (cached after first call)
- **Query generation (ChatGPT):** ~1-2s per query
- **Query execution:** Varies by query complexity
- **Claim creation:** ~500ms (includes intermediary nodes)
- **Claim promotion:** ~300ms additional (atomic Cypher transaction)

## Production Consideration

This agent is suitable for:
- Testing and validation
- Small batch claim submissions (<100 claims/minute)
- Interactive query exploration
- Prototyping agent workflows

For production use, consider:
- Adding rate limiting on ChatGPT calls
- Batch claim submission endpoint (FastAPI)
- Query result caching
- MCP server integration for Copilot access
- Error recovery and retry logic

## See Also
- [QUERY_EXECUTOR_AGENT_PROMPT.md](../../md/Agents/QUERY_EXECUTOR_AGENT_PROMPT.md) - System prompt for agent role
- [claim_ingestion_pipeline.py](../tools/claim_ingestion_pipeline.py) - Pipeline implementation
- [requirements.txt](../agents/requirements.txt) - Python dependencies
